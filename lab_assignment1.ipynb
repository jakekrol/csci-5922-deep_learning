{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg8BSVK4OcFx"
      },
      "source": [
        "# Lab Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pit0BSlkOfcw"
      },
      "source": [
        "Student name: [fill in]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXp58l3vrC1L"
      },
      "source": [
        "## Notebook version\n",
        "\n",
        "This notebook includes all the codes in the codebase of lab assignment 1. Completing and submitting this script is equivalent to submitting the codebase. Please note that your submitted script should include errorless cell outputs that contain necessary information that proves you have successfully run the notebook in your own directory.\n",
        "\n",
        "You can choose to (1) run this notebook locally on your end or (2) run this notebook on colab. For the former, you will need to download the dataset to your device that resembles the instructions for the codebase. For the latter, **you will need to upload the dataset to your Google Drive** account, and connect your colab notebook to your Google Drive. Then, go to \"File->Save a copy in Drive\" to create a copy you can edit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWOk8c6QstJ2"
      },
      "source": [
        "#### Colab (if applicable)\n",
        "\n",
        "If you are running this script on colab, uncomment and run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OATj2nvHs2O1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/home/jake/ghub/csci-5922-deep_learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHvOW4Qxs30Y"
      },
      "source": [
        "Note that the Google Drive directory has the root `/content/drive/`. For instance, my directory to the dataset is `'/content/drive/My Drive/Courses/CSCI 5922/CSCI 5922 SP25/Demo/MNIST/'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ufLsFPnq6gu"
      },
      "source": [
        "### mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nqaf3LuXOa1c"
      },
      "outputs": [],
      "source": [
        "#Original source: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
        "#It has been modified for ease of use w/ pytorch\n",
        "\n",
        "#You do NOT need to modify ANY code in this file!\n",
        "\n",
        "import numpy as np\n",
        "import struct\n",
        "from array import array\n",
        "import torch\n",
        "\n",
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
        "                 test_images_filepath, test_labels_filepath):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        n = 60000 if \"train\" in images_filepath else 10000\n",
        "        labels = torch.zeros((n, 10))\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            l = torch.tensor(array(\"B\", file.read())).unsqueeze(-1)\n",
        "            l = torch.concatenate((torch.arange(0, n).unsqueeze(-1), l), dim = 1).type(torch.int32)\n",
        "            labels[l[:,0], l[:,1]] = 1\n",
        "\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = torch.zeros((n, 28**2))\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            #img = img.reshape(28, 28)\n",
        "            images[i, :] = torch.tensor(img)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        return (x_train, y_train),(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpKgf2fMquMh"
      },
      "source": [
        "### activations.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WuJUuwXrOoVg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ReLU():\n",
        "    #Complete this class\n",
        "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
        "        #implement ReLU(x) here\n",
        "         return torch.maximum(torch.tensor(0.0), x)\n",
        "\n",
        "    def backward(self,delta: torch.tensor, x: torch.tensor) -> torch.tensor:\n",
        "        #implement delta * ReLU'(x) here\n",
        "        return delta * torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
        "\n",
        "class LeakyReLU():\n",
        "    #Complete this class\n",
        "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
        "        #implement LeakyReLU(x) here\n",
        "        return torch.maximum(0.1 * x, x)\n",
        "\n",
        "    def backward(self,delta: torch.tensor, x: torch.tensor) -> torch.tensor:\n",
        "        #implement delta * LeakyReLU'(x) here\n",
        "        return delta * torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZsFi5HUVvWm",
        "outputId": "1bfcde53-04de-4d97-f132-bb846e483260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 1.])\n",
            "tensor([0., 0., 3.])\n"
          ]
        }
      ],
      "source": [
        "r = ReLU()\n",
        "x = torch.tensor([-1.0, 0.0, 1.0])\n",
        "print(r.forward(x))\n",
        "delta = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(r.backward(delta, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2zEHN7qxuh"
      },
      "source": [
        "### framework.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xjBDqIScO-hy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded and normalized.\n",
            "Model initialized and hyperparameters set.\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                   | 0/117 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "shape torch.Size([256, 10]) torch.Size([256, 10])\n",
            "Train Loss: tensor(0.0198, device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MLP:\n",
        "    '''\n",
        "    This class should implement a generic MLP learning framework. The core structure of the program has been provided for you.\n",
        "    But, you need to complete the following functions:\n",
        "    1: initialize()\n",
        "    2: forward(), including activations\n",
        "    3: backward(), including activations\n",
        "    4: TrainMLP()\n",
        "    '''\n",
        "    def __init__(self, layer_sizes: list[int], device: torch.device):\n",
        "\n",
        "        # Storage for model parameters\n",
        "        self.layer_sizes: list[int] = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "        self.weights: list[torch.tensor] = []\n",
        "        self.biases: list[torch.tensor] = []\n",
        "        self.device = device\n",
        "\n",
        "        # Temporary data\n",
        "        self.features = {}\n",
        "\n",
        "    def set_hp(self, lr: float, bs: int, activation: object) -> None:\n",
        "        self.learning_rate = lr\n",
        "        self.batch_size = bs\n",
        "        self.activation_function = activation\n",
        "\n",
        "    def initialize(self):\n",
        "        for i in range(len(self.layer_sizes) - 1): # create l-1 weight matrices (l includes input layer)\n",
        "            d_in = self.layer_sizes[i]\n",
        "            d_out = self.layer_sizes[i + 1]\n",
        "            weight = torch.empty(d_in, d_out, device=self.device)\n",
        "            torch.nn.init.uniform_(weight, -np.sqrt(6 / (d_in + d_out)), np.sqrt(6 / (d_in + d_out)))\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(torch.zeros(d_out, device=self.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "        \n",
        "        for i in range(len(self.layer_sizes) - 2): # loop through hidden layers (l-2, including input layer)\n",
        "            # net input\n",
        "            z = x @ self.weights[i] + self.biases[i]\n",
        "            self.features[f'h_{i}'] = {'z': z}\n",
        "            # activation\n",
        "            a = self.activation_function.forward(z)\n",
        "            self.features[f'h_{i}']['a'] = a\n",
        "            # update x\n",
        "            x = a\n",
        "        # output layer\n",
        "        z = x @ self.weights[-1] + self.biases[-1]\n",
        "        self.features[f'z_o'] = {'z': z}\n",
        "        # not sure why we do ReLU before softmax\n",
        "        a = self.activation_function.forward(z)\n",
        "        self.features[f'a_o'] = {'a': a}\n",
        "        yhat = torch.softmax(z, dim=1)\n",
        "        self.features[f'yhat'] = yhat\n",
        "        return yhat\n",
        "\n",
        "    def backward(self, delta: torch.tensor) -> None:\n",
        "        #Complete this function\n",
        "\n",
        "        '''\n",
        "        This function should backpropagate the provided delta through the entire MLP, and update the weights according to the hyper-parameters\n",
        "        stored in the class variables.\n",
        "        '''\n",
        "        return\n",
        "\n",
        "\n",
        "def TrainMLP(model: MLP, x_train: torch.tensor, y_train: torch.tensor) -> MLP:\n",
        "    #Complete this function\n",
        "\n",
        "    '''\n",
        "    This function should train the MLP for 1 epoch, using the provided data and forward/backward propagating as necessary.\n",
        "    '''\n",
        "\n",
        "    #set up a random sampling of the data\n",
        "    bs = model.batch_size\n",
        "    N = x_train.shape[0]\n",
        "    rng = np.random.default_rng()\n",
        "    idx = rng.permutation(N)\n",
        "\n",
        "    #variable to accumulate total loss over the epoch\n",
        "    L = 0\n",
        "\n",
        "    for i in tqdm.tqdm(range(N // bs)):\n",
        "        x = x_train[idx[i * bs:(i + 1) * bs], ...]\n",
        "        y = y_train[idx[i * bs:(i + 1) * bs], ...]\n",
        "\n",
        "        #forward propagate and compute loss (l) here\n",
        "        y_hat = model.forward(x)\n",
        "        p = torch.exp(y_hat)\n",
        "        p /= torch.sum(p, dim = 1, keepdim = True)\n",
        "        l = -1 * torch.sum(y * torch.log(p))\n",
        "        L += l\n",
        "        print(i)\n",
        "\n",
        "        # #backpropagate here\n",
        "        # delta = p - y # b x output\n",
        "        # # grad relu\n",
        "        # delta_aout = model.activation_function.backward(delta, model.features['z_o']['z']) # b x output\n",
        "        # # gradient of matrix product w.r.t. matrix is the other matrix transposed\n",
        "        # delta_w2 = model.features['h_0']['a'].T @ delta_aout # h1 x output \n",
        "        # # update w2\n",
        "        # model.weights[-1] = model.weights[-1] - (model.learning_rate * delta_w2)\n",
        "        # # gradient of bias is just the delta\n",
        "        # delta_b2 = torch.sum(delta_aout, dim=0)  \n",
        "        # # update b2\n",
        "        # model.biases[-1] = model.biases[-1] - (model.learning_rate * delta_b2)\n",
        "        # # now do the same for the hidden layer parameters\n",
        "        # # delta for hidden layer\n",
        "        # # need to sum over the output dimension\n",
        "        # delta_a1 \n",
        "\n",
        "        # delta_a1 = model.activation_function\n",
        "        # break\n",
        "        # self.weights[-1] = self.weights[-1] - (model.learning_rate * delta_w2)\n",
        "        # # bias gradient is just the delta\n",
        "        # break\n",
        "\n",
        "        grads_w = [None] * len(model.weights)\n",
        "        grads_b = [None] * len(model.biases)\n",
        "        \n",
        "        # Compute gradient of loss w.r.t. logits\n",
        "        y_pred = self.features[-1]\n",
        "        delta = y_pred - y_true  # dCE/dO\n",
        "        \n",
        "        # Backpropagate through layers\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w[i] = self.features[i].T @ delta / y_true.shape[0]\n",
        "            grads_b[i] = delta.mean(dim=0, keepdim=True)\n",
        "            \n",
        "            if i > 0:\n",
        "                delta = (delta @ self.weights[i].T) * (self.features[i] > 0).float()  # ReLU derivative\n",
        "        \n",
        "        # Update parameters\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
        "            self.biases[i] -= self.learning_rate * grads_b[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Train Loss:\", L / ((N // bs) * bs))\n",
        "\n",
        "\n",
        "def TestMLP(model: MLP, x_test: torch.tensor, y_test: torch.tensor) -> tuple[float, float]:\n",
        "    bs = model.batch_size\n",
        "    N = x_test.shape[0]\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    idx = rng.permutation(N)\n",
        "\n",
        "    L = 0\n",
        "    A = 0\n",
        "\n",
        "    for i in tqdm.tqdm(range(N // bs)):\n",
        "        x = x_test[idx[i * bs:(i + 1) * bs], ...]\n",
        "        y = y_test[idx[i * bs:(i + 1) * bs], ...]\n",
        "\n",
        "        y_hat = model.forward(x)\n",
        "        p = torch.exp(y_hat)\n",
        "        p /= torch.sum(p, dim = 1, keepdim = True)\n",
        "        l = -1 * torch.sum(y * torch.log(p))\n",
        "        L += l\n",
        "\n",
        "        A += torch.sum(torch.where(torch.argmax(p, dim = 1) == torch.argmax(y, dim = 1), 1, 0))\n",
        "\n",
        "    print(\"Test Loss:\", L / ((N // bs) * bs), \"Test Accuracy: {:.2f}%\".format(100 * A / ((N // bs) * bs)))\n",
        "\n",
        "def normalize_mnist() -> tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n",
        "    '''\n",
        "    This function loads the MNIST dataset, then normalizes the \"X\" values to have zero mean, unit variance.\n",
        "    '''\n",
        "\n",
        "    base_path = 'mnist/'\n",
        "    mnist = MnistDataloader(base_path + \"train-images.idx3-ubyte\", base_path + \"train-labels.idx1-ubyte\",\n",
        "                            base_path + \"t10k-images.idx3-ubyte\", base_path + \"t10k-labels.idx1-ubyte\")\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_mean = torch.mean(x_train, dim=0, keepdim=True)\n",
        "    x_std = torch.std(x_train, dim=0, keepdim=True)\n",
        "\n",
        "    x_train -= x_mean\n",
        "    x_train /= x_std\n",
        "    x_train[x_train != x_train] = 0\n",
        "\n",
        "    x_test -= x_mean\n",
        "    x_test /= x_std\n",
        "    x_test[x_test != x_test] = 0\n",
        "\n",
        "    return x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
        "\n",
        "def main():\n",
        "    '''\n",
        "    This is an example of how to use the framework when completed. You can build off of this code to design your experiments for part 2.\n",
        "    '''\n",
        "\n",
        "    x_train, y_train, x_test, y_test = normalize_mnist()\n",
        "    print(\"Data loaded and normalized.\")\n",
        "\n",
        "    '''\n",
        "    For the experiment, adjust the list [784,...,10] as desired to test other architectures.\n",
        "    You are encouraged to play around with any of the following values if you so desire:\n",
        "    E, lr, bs, activation\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MLP([784, 256, 10], device)\n",
        "    model.initialize()\n",
        "    model.set_hp(lr=1e-6, bs=512, activation=ReLU())\n",
        "    print(\"Model initialized and hyperparameters set.\")\n",
        "\n",
        "    # E = 1\n",
        "    E = 1\n",
        "    for epoch in range(E):\n",
        "        print(f\"Epoch {epoch+1}/{E}\")\n",
        "        TrainMLP(model, x_train, y_train)\n",
        "        # TestMLP(model, x_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "m = MLP([784, 256, 10], device=device)\n",
        "m.initialize()\n",
        "len(m.weights)\n",
        "for i in range(0,len(m.layer_sizes)-1):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0000)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.log(torch.tensor(torch.e))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
