{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg8BSVK4OcFx"
      },
      "source": [
        "# Lab Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pit0BSlkOfcw"
      },
      "source": [
        "Student name: [fill in]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXp58l3vrC1L"
      },
      "source": [
        "## Notebook version\n",
        "\n",
        "This notebook includes all the codes in the codebase of lab assignment 1. Completing and submitting this script is equivalent to submitting the codebase. Please note that your submitted script should include errorless cell outputs that contain necessary information that proves you have successfully run the notebook in your own directory.\n",
        "\n",
        "You can choose to (1) run this notebook locally on your end or (2) run this notebook on colab. For the former, you will need to download the dataset to your device that resembles the instructions for the codebase. For the latter, **you will need to upload the dataset to your Google Drive** account, and connect your colab notebook to your Google Drive. Then, go to \"File->Save a copy in Drive\" to create a copy you can edit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWOk8c6QstJ2"
      },
      "source": [
        "#### Colab (if applicable)\n",
        "\n",
        "If you are running this script on colab, uncomment and run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OATj2nvHs2O1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/home/jake/ghub/csci-5922-deep_learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHvOW4Qxs30Y"
      },
      "source": [
        "Note that the Google Drive directory has the root `/content/drive/`. For instance, my directory to the dataset is `'/content/drive/My Drive/Courses/CSCI 5922/CSCI 5922 SP25/Demo/MNIST/'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ufLsFPnq6gu"
      },
      "source": [
        "### mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nqaf3LuXOa1c"
      },
      "outputs": [],
      "source": [
        "#Original source: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
        "#It has been modified for ease of use w/ pytorch\n",
        "\n",
        "#You do NOT need to modify ANY code in this file!\n",
        "\n",
        "import numpy as np\n",
        "import struct\n",
        "from array import array\n",
        "import torch\n",
        "\n",
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
        "                 test_images_filepath, test_labels_filepath):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        n = 60000 if \"train\" in images_filepath else 10000\n",
        "        labels = torch.zeros((n, 10))\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            l = torch.tensor(array(\"B\", file.read())).unsqueeze(-1)\n",
        "            l = torch.concatenate((torch.arange(0, n).unsqueeze(-1), l), dim = 1).type(torch.int32)\n",
        "            labels[l[:,0], l[:,1]] = 1\n",
        "\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = torch.zeros((n, 28**2))\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            #img = img.reshape(28, 28)\n",
        "            images[i, :] = torch.tensor(img)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        return (x_train, y_train),(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpKgf2fMquMh"
      },
      "source": [
        "### activations.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WuJUuwXrOoVg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ReLU():\n",
        "    #Complete this class\n",
        "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
        "         return torch.maximum(torch.tensor(0.0), x)\n",
        "\n",
        "    def backward(self,delta: torch.tensor, x: torch.tensor) -> torch.tensor:\n",
        "        #implement delta * ReLU'(x) here\n",
        "        return delta * (x > 0).float()\n",
        "\n",
        "class LeakyReLU():\n",
        "    #Complete this class\n",
        "    def forward(self,x: torch.tensor) -> torch.tensor:\n",
        "        #implement LeakyReLU(x) here\n",
        "        return torch.where(x > 0, x, 0.1 * x)\n",
        "\n",
        "    def backward(self,delta: torch.tensor, x: torch.tensor) -> torch.tensor:\n",
        "        #implement delta * LeakyReLU'(x) here\n",
        "        delta * torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZsFi5HUVvWm",
        "outputId": "1bfcde53-04de-4d97-f132-bb846e483260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.2000,  3.0000])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l = LeakyReLU()\n",
        "l.forward(torch.tensor([-2,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2zEHN7qxuh"
      },
      "source": [
        "### framework.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xjBDqIScO-hy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded and normalized.\n",
            "Model initialized and hyperparameters set.\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                              | 0/117 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (10) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 185\u001b[0m\n\u001b[1;32m    182\u001b[0m         TestMLP(model, x_test, y_test)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[52], line 181\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(E):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m     \u001b[43mTrainMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     TestMLP(model, x_test, y_test)\n",
            "Cell \u001b[0;32mIn[52], line 105\u001b[0m, in \u001b[0;36mTrainMLP\u001b[0;34m(model, x_train, y_train)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m#backpropagate here\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     delta \u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m-\u001b[39m y\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, L \u001b[38;5;241m/\u001b[39m ((N \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m bs) \u001b[38;5;241m*\u001b[39m bs))\n",
            "Cell \u001b[0;32mIn[52], line 70\u001b[0m, in \u001b[0;36mMLP.backward\u001b[0;34m(self, delta)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         delta \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (delta\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m delta\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (10) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MLP:\n",
        "    '''\n",
        "    This class should implement a generic MLP learning framework. The core structure of the program has been provided for you.\n",
        "    But, you need to complete the following functions:\n",
        "    1: initialize()\n",
        "    2: forward(), including activations\n",
        "    3: backward(), including activations\n",
        "    4: TrainMLP()\n",
        "    '''\n",
        "    def __init__(self, layer_sizes: list[int], device: torch.device):\n",
        "\n",
        "        # Storage for model parameters\n",
        "        self.layer_sizes: list[int] = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "        self.weights: list[torch.tensor] = []\n",
        "        self.biases: list[torch.tensor] = []\n",
        "        self.device = device\n",
        "\n",
        "        # Temporary data\n",
        "        self.features = {}\n",
        "\n",
        "    def set_hp(self, lr: float, bs: int, activation: object) -> None:\n",
        "        self.learning_rate = lr\n",
        "        self.batch_size = bs\n",
        "        self.activation_function = activation\n",
        "\n",
        "    def initialize(self):\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            d_in = self.layer_sizes[i]\n",
        "            d_out = self.layer_sizes[i + 1]\n",
        "            weight = torch.empty(d_out, d_in, device=self.device)\n",
        "            torch.nn.init.uniform_(weight, -np.sqrt(6 / (d_in + d_out)), np.sqrt(6 / (d_in + d_out)))\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(torch.zeros(d_out, device=self.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = x @ self.weights[i].T + self.biases[i]\n",
        "            self.features[f'h_{i}'] = {'z': z}\n",
        "            a = self.activation_function.forward(z)\n",
        "            self.features[f'h_{i}']['a'] = a\n",
        "            x = a\n",
        "        # output layer\n",
        "        z = x @ self.weights[-1].T + self.biases[-1]\n",
        "        self.features[f'h_{len(self.weights) - 1}'] = {'z': z}\n",
        "        a = torch.softmax(z, dim=1)\n",
        "        self.features[f'h_{len(self.weights) - 1}']['a'] = a\n",
        "        return a\n",
        "\n",
        "    def backward(self, delta: torch.tensor) -> None:\n",
        "        #Complete this function\n",
        "\n",
        "        '''\n",
        "        This function should backpropagate the provided delta through the entire MLP, and update the weights according to the hyper-parameters\n",
        "        stored in the class variables.\n",
        "        '''\n",
        "        return\n",
        "\n",
        "\n",
        "def TrainMLP(model: MLP, x_train: torch.tensor, y_train: torch.tensor) -> MLP:\n",
        "    #Complete this function\n",
        "\n",
        "    '''\n",
        "    This function should train the MLP for 1 epoch, using the provided data and forward/backward propagating as necessary.\n",
        "    '''\n",
        "\n",
        "    #set up a random sampling of the data\n",
        "    bs = model.batch_size\n",
        "    N = x_train.shape[0]\n",
        "    rng = np.random.default_rng()\n",
        "    idx = rng.permutation(N)\n",
        "\n",
        "    #variable to accumulate total loss over the epoch\n",
        "    L = 0\n",
        "\n",
        "    for i in tqdm.tqdm(range(N // bs)):\n",
        "        x = x_train[idx[i * bs:(i + 1) * bs], ...]\n",
        "        y = y_train[idx[i * bs:(i + 1) * bs], ...]\n",
        "\n",
        "        #forward propagate and compute loss (l) here\n",
        "        y_hat = model.forward(x)\n",
        "        p = torch.exp(y_hat)\n",
        "        p /= torch.sum(p, dim = 1, keepdim = True)\n",
        "        l = -1 * torch.sum(y * torch.log(p))\n",
        "        L += l\n",
        "        print(i)\n",
        "\n",
        "        #backpropagate here\n",
        "        delta = p - y\n",
        "\n",
        "    print(\"Train Loss:\", L / ((N // bs) * bs))\n",
        "\n",
        "\n",
        "def TestMLP(model: MLP, x_test: torch.tensor, y_test: torch.tensor) -> tuple[float, float]:\n",
        "    bs = model.batch_size\n",
        "    N = x_test.shape[0]\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    idx = rng.permutation(N)\n",
        "\n",
        "    L = 0\n",
        "    A = 0\n",
        "\n",
        "    for i in tqdm.tqdm(range(N // bs)):\n",
        "        x = x_test[idx[i * bs:(i + 1) * bs], ...]\n",
        "        y = y_test[idx[i * bs:(i + 1) * bs], ...]\n",
        "\n",
        "        y_hat = model.forward(x)\n",
        "        p = torch.exp(y_hat)\n",
        "        p /= torch.sum(p, dim = 1, keepdim = True)\n",
        "        l = -1 * torch.sum(y * torch.log(p))\n",
        "        L += l\n",
        "\n",
        "        A += torch.sum(torch.where(torch.argmax(p, dim = 1) == torch.argmax(y, dim = 1), 1, 0))\n",
        "\n",
        "    print(\"Test Loss:\", L / ((N // bs) * bs), \"Test Accuracy: {:.2f}%\".format(100 * A / ((N // bs) * bs)))\n",
        "\n",
        "def normalize_mnist() -> tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n",
        "    '''\n",
        "    This function loads the MNIST dataset, then normalizes the \"X\" values to have zero mean, unit variance.\n",
        "    '''\n",
        "\n",
        "    base_path = 'mnist/'\n",
        "    mnist = MnistDataloader(base_path + \"train-images.idx3-ubyte\", base_path + \"train-labels.idx1-ubyte\",\n",
        "                            base_path + \"t10k-images.idx3-ubyte\", base_path + \"t10k-labels.idx1-ubyte\")\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_mean = torch.mean(x_train, dim=0, keepdim=True)\n",
        "    x_std = torch.std(x_train, dim=0, keepdim=True)\n",
        "\n",
        "    x_train -= x_mean\n",
        "    x_train /= x_std\n",
        "    x_train[x_train != x_train] = 0\n",
        "\n",
        "    x_test -= x_mean\n",
        "    x_test /= x_std\n",
        "    x_test[x_test != x_test] = 0\n",
        "\n",
        "    return x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
        "\n",
        "def main():\n",
        "    '''\n",
        "    This is an example of how to use the framework when completed. You can build off of this code to design your experiments for part 2.\n",
        "    '''\n",
        "\n",
        "    x_train, y_train, x_test, y_test = normalize_mnist()\n",
        "    print(\"Data loaded and normalized.\")\n",
        "\n",
        "    '''\n",
        "    For the experiment, adjust the list [784,...,10] as desired to test other architectures.\n",
        "    You are encouraged to play around with any of the following values if you so desire:\n",
        "    E, lr, bs, activation\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MLP([784, 256, 10], device)\n",
        "    model.initialize()\n",
        "    model.set_hp(lr=1e-6, bs=512, activation=ReLU())\n",
        "    print(\"Model initialized and hyperparameters set.\")\n",
        "\n",
        "    # E = 1\n",
        "    E = 25\n",
        "    for epoch in range(E):\n",
        "        print(f\"Epoch {epoch+1}/{E}\")\n",
        "        TrainMLP(model, x_train, y_train)\n",
        "        TestMLP(model, x_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initialize\n",
            "torch.Size([256, 10])\n"
          ]
        }
      ],
      "source": [
        "m = MLP([784, 256, 10])\n",
        "m.initialize()\n",
        "print(m.weights[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0000)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.log(torch.tensor(torch.e))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
